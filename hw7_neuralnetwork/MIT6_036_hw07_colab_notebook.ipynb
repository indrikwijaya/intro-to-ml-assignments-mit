{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT6.036 hw07 colab notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIaEwCD406A"
      },
      "source": [
        "#MIT 6.036 Spring 2019: Homework 7#\n",
        "\n",
        "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YM-_zLf9Bp-"
      },
      "source": [
        "#!rm -rf code_for_hw7*\n",
        "#!rm -rf mnist\n",
        "#!wget --quiet https://introml_oll.odl.mit.edu/6.036/static/homework/hw07/code_for_hw7.zip\n",
        "#!unzip code_for_hw7.zip\n",
        "#!mv code_for_hw7/* .\n",
        "\n",
        "from code_for_hw7 import *\n",
        "import numpy as np\n",
        "import modules_disp as disp"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFxhrJ5XDlvb"
      },
      "source": [
        "# 2) Implementing Neural Networks\n",
        "\n",
        "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
        "\n",
        "<br>\n",
        "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
        "\n",
        "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
        "\n",
        "<br>\n",
        "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjQgtwPHj08n"
      },
      "source": [
        "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
        "\n",
        "```\n",
        "# build a 3-layer network\n",
        "net = Sequential([Linear(2,3), Tanh(),\n",
        "                  Linear(3,3), Tanh(),\n",
        "    \t          Linear(3,2), SoftMax()])\n",
        "# train the network on data and labels\n",
        "net.sgd(X, Y)\n",
        "```\n",
        "Please fill in any unimplemented methods below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwpgsbnho9K"
      },
      "source": [
        "## Linear Modules: ##\n",
        "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
        "\n",
        "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VsYLAxCfy7U"
      },
      "source": [
        "class Linear(Module):\n",
        "    def __init__(self, m, n):\n",
        "        self.m, self.n = (m, n)  # (in size, out size)\n",
        "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
        "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
        "\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.dot(self.W.T, A) + self.W0  # (m x n)^T (m x b) = (n x b)\n",
        "\n",
        "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
        "        self.dLdW = np.dot(self.A, dLdZ.T)                  # (m x n)\n",
        "        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n",
        "        return np.dot(self.W, dLdZ)                         # (m x b)\n",
        "\n",
        "    def sgd_step(self, lrate):  # Gradient descent step\n",
        "        self.W -= lrate*self.dLdW\n",
        "        self.W0 -= lrate*self.dLdW0"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqZ7_kZYr5s5"
      },
      "source": [
        " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3yePY0r4eA",
        "outputId": "ee06312d-1b3c-47b8-a813-fe933412387f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# data\n",
        "X, Y = super_simple_separable()\n",
        "\n",
        "# module\n",
        "linear_1 = Linear(2, 3)\n",
        "\n",
        "#hyperparameters\n",
        "lrate = 0.005\n",
        "\n",
        "# test case\n",
        "# forward\n",
        "z_1 = linear_1.forward(X)\n",
        "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
        "                    [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
        "                    [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
        "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
        "\n",
        "# backward\n",
        "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
        "                                    [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
        "                                    [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
        "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
        "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
        "dLdX = linear_1.backward(dL_dz1)\n",
        "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
        "\n",
        "# sgd step\n",
        "linear_1.sgd_step(lrate)\n",
        "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
        "                          [1.58455079, 1.32055711, -0.69218045]]),\n",
        "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
        "\n",
        "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
        "                            [-2.90968033e-06],\n",
        "                            [-1.01331631e-03]]),\n",
        "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear_forward: OK\n",
            "linear_backward: OK\n",
            "linear_sgd_step_W: OK\n",
            "linear_sgd_step_W0: OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETL01mPsBz4"
      },
      "source": [
        "The following datasets are defined for your use:\n",
        "*  `super_simple_separable_through_origin()`\n",
        "*  `super_simple_separable()`\n",
        "*  `xor()`\n",
        "*  `xor_more()`\n",
        "*  `hard()`\n",
        "\n",
        "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
        "```\n",
        "def plot_nn(X, Y, nn):\n",
        "    \"\"\" Plot output of nn vs. data \"\"\"\n",
        "    def predict(x):\n",
        "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
        "    xmin, ymin = np.min(X, axis=1)-1\n",
        "    xmax, ymax = np.max(X, axis=1)+1\n",
        "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
        "    plot_data(X, Y, nax)\n",
        "    plt.show()```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s70beWJh09h"
      },
      "source": [
        "## Activation functions: ##\n",
        "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
        "\n",
        "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwaNAtLnhenT"
      },
      "source": [
        "### Tanh: ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6eD3dnftiR"
      },
      "source": [
        "class Tanh(Module):            # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.tanh(Z)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # Uses stored self.A\n",
        "        return dLdA*(1 - (self.A**2))            # Your code: return dLdZ (?, b)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FW7ocKRhcgY"
      },
      "source": [
        "### ReLU: ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fm2KsLUfqdp"
      },
      "source": [
        "class ReLU(Module):              # Layer activation\n",
        "    def forward(self, Z):\n",
        "        self.A = np.maximum(0, Z)            # Your code: (?, b)\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dLdA):    # uses stored self.A\n",
        "        return dLdA*(self.A != 0)         # Your code: return dLdZ (?, b)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKtXuTQ0hSNO"
      },
      "source": [
        "###SoftMax: ###\n",
        "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqK-CJrnfn22"
      },
      "source": [
        "class SoftMax(Module):           # Output activation\n",
        "    def forward(self, Z):\n",
        "        return np.exp(Z)/np.sum(np.exp(Z), axis=0)              # Your code: (?, b)\n",
        "\n",
        "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
        "        return dLdZ\n",
        "\n",
        "    def class_fun(self, Ypred):  # Return class indices\n",
        "        return np.argmax(Ypred, axis=0)              # Your code: (1, b)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZc7HnMSh4fn"
      },
      "source": [
        "## Loss Functions:##\n",
        "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
        "\n",
        "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uy0pHVhNd8"
      },
      "source": [
        "### NLL: ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Fb8mimflgb"
      },
      "source": [
        "class NLL(Module):       # Loss\n",
        "    def forward(self, Ypred, Y):\n",
        "        self.Ypred = Ypred\n",
        "        self.Y = Y\n",
        "        return float(np.sum(-Y*np.log(Ypred)))      # Your code\n",
        "\n",
        "    def backward(self):  # Use stored self.Ypred, self.Y\n",
        "        return self.Ypred - self.Y      # Your code"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EffzDFkqMX"
      },
      "source": [
        "## Activation and Loss Test Cases: ##\n",
        "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DJFzpahkvcD",
        "outputId": "bfebae17-9887-4cdc-fbcd-ac5a11f44947",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear_1.W: OK\n",
            "linear_1.W0: OK\n",
            "linear_2.W: OK\n",
            "linear_2.W0: OK\n",
            "z_1: OK\n",
            "a_1: OK\n",
            "z_2: OK\n",
            "a_2: OK\n",
            "loss: OK\n",
            "dloss: OK\n",
            "dL_dz2: OK\n",
            "dL_da1: OK\n",
            "dL_dz1: OK\n",
            "dL_dX: OK\n",
            "updated_linear_1.W: OK\n",
            "updated_linear_1.W0: OK\n",
            "updated_linear_2.W: OK\n",
            "updated_linear_2.W0: OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0dXg-Qk05_",
        "outputId": "2f7a80cd-5cb7-4051-eaa4-d13a087b7461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
        "np.random.seed(0)\n",
        "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear_1.W: OK\n",
            "linear_1.W0: OK\n",
            "linear_2.W: OK\n",
            "linear_2.W0: OK\n",
            "z_1: OK\n",
            "a_1: OK\n",
            "z_2: OK\n",
            "a_2: OK\n",
            "loss: OK\n",
            "dloss: OK\n",
            "dL_dz2: OK\n",
            "dL_da1: OK\n",
            "dL_dz1: OK\n",
            "dL_dX: OK\n",
            "updated_linear_1.W: OK\n",
            "updated_linear_1.W0: OK\n",
            "updated_linear_2.W: OK\n",
            "updated_linear_2.W0: OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5JgBU2iBCZ"
      },
      "source": [
        "## Neural Network: ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMGcdnXgiF3"
      },
      "source": [
        "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
        "\n",
        "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejO15Vr7fhKB"
      },
      "source": [
        " class Sequential:\n",
        "    def __init__(self, modules, loss):            # List of modules, loss module\n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
        "        D, N = X.shape\n",
        "        sum_loss = 0\n",
        "        for it in range(iters):\n",
        "          i = np.random.randint(N)\n",
        "          Xt = X[:, i:i+1]\n",
        "          Yt = Y[:, i:i+1]\n",
        "          Ypred = self.forward(Xt)\n",
        "          sum_loss += self.loss.forward(Ypred, Yt)\n",
        "          err = self.loss.backward()\n",
        "          self.backward(err)\n",
        "          self.sgd_step(lrate)\n",
        "                             # Your code\n",
        "\n",
        "    def forward(self, Xt):                        # Compute Ypred\n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
        "        # Note reversed list of modules\n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def sgd_step(self, lrate):                    # Gradient descent step\n",
        "        for m in self.modules: m.sgd_step(lrate)\n",
        "\n",
        "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
        "        # Utility method to print accuracy on full dataset, should\n",
        "        # improve over time when doing SGD. Also prints current loss,\n",
        "        # which should decrease over time. Call this on each iteration\n",
        "        # of SGD!\n",
        "        if it % every == 1:\n",
        "            cf = self.modules[-1].class_fun\n",
        "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
        "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUojaXqphDjh"
      },
      "source": [
        "## Neural Network / SGD Test Cases: ##\n",
        "Use Test 3 and Test 4 to help you debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmupM8OScodw",
        "outputId": "dd42a3fe-d9e9-4b9b-d9e7-f7d09314f118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
        "X, Y = hard()\n",
        "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
        "disp.classify(X, Y, nn, it=100000)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Sequential at 0x7ff5996efdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaWfgC7Qe3ar"
      },
      "source": [
        "# TEST 4: try calling these methods that train with a simple dataset\n",
        "def nn_tanh_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_relu_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
        "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
        "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
        "\n",
        "\n",
        "def nn_pred_test():\n",
        "    np.random.seed(0)\n",
        "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
        "    X, Y = super_simple_separable()\n",
        "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
        "    Ypred = nn.forward(X)\n",
        "\n",
        "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo_woDFh3a2v",
        "outputId": "6d6ef994-1de1-4f95-d3c0-7b3df0c299d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn_pred_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# ([0, 0, 0, 0], [8.56575061835767])\n",
        "# '''"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 0, 0, 0], [8.565750618357672])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dx-zM2y3R0z",
        "outputId": "1f306ac6-724d-43bc-bbd1-0df9d3d385e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn_tanh_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
        "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
        "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
        "#  [[0.544808855557535, -0.08366117689965663],\n",
        "#   [-0.06331837550937104, 0.24078409926389266],\n",
        "#   [0.08677202043839037, 0.8360167748667923],\n",
        "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
        "# '''"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
              "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
              "  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
              " [[0.544808855557535, -0.08366117689965663],\n",
              "  [-0.06331837550937104, 0.24078409926389266],\n",
              "  [0.08677202043839037, 0.8360167748667923],\n",
              "  [-0.0037249480614718, 0.0037249480614718]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmYT9IWk3TQL",
        "outputId": "a323d597-aee5-4647-9919-5bc1d612fce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn_relu_test()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
        "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
        "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
        "#  [[0.501769700845158, -0.040622022187279644],\n",
        "#   [-0.09260786974986723, 0.27007359350438886],\n",
        "#   [0.08364438851530624, 0.8391444067898763],\n",
        "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
        "# '''"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
              "  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
              "  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
              " [[0.501769700845158, -0.040622022187279644],\n",
              "  [-0.09260786974986723, 0.27007359350438886],\n",
              "  [0.08364438851530624, 0.8391444067898763],\n",
              "  [-0.004252310922204504, 0.004252310922204505]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtY1UP5uf_W7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}